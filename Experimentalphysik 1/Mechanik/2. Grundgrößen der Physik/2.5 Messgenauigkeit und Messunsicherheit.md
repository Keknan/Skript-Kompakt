***

Wir als Physiker wollen Größen so genau wie möglich bestimmen. Das geht leider nicht zu 100%, es wird immer ein paar **Abweichungen** geben, die wir Messunsicherheit nennen. Es gibt zwei Arten:

1) Systematische Abweichungen: Wir messen immer wieder und sehen, dass unser gewünschter Wert in eine Richtung verschoben ist
2) Statistische Abweichungen: Zufällig auftretende Abweichungen. Diese richten sich nach einer Wahrscheinlichkeitsverteilung

Ein anschauliches Beispiel wäre ein professioneller Schütze. Dieser Schütze hat nun aber ein Gewehr, das einen kleinen drift nach links besitzt. Wenn dieser nun auf ein Ziel schießt, wird er eigentlich mit perfekter Präzision schießen. Die Kugeln werden aber immer nach links streuen. Das wäre ein systematischer Fehler.
Würde ich - ein Laie - mit einer perfekten Waffe schießen, dann würden die Kugeln wahrscheinlich irgendwo treffen. Dies wäre ein statistischer Fehler, weil ich nicht weiß, wie man richtig schießt.

Wenn wir nun also eine Größe, wie die Erdbeschleunigung angeben wollen, geben wir ihre beste Schätzung, den **Mittelwert** $\bar{G}$ an, als auch unsere **Unsicherheit** $\Delta G$. Diese ist ein Maß für die Streuung um den richtigen Wert. Wichtig:

>**Eine Messung ohne Angabe der Messunsicherheit ist wertlos**

Unsere Messgröße $G$ schreibt man also

$$
G=\bar{G}+\Delta G
$$

## 2.5.2 Statistische Abweichungen
***
(Ich habe 2.5.1 übersprungen, da dort nur mein Schützenbeispiel erläutert wurde)

Messen wir nun eine Messgröße $G$ $n$-mal (uns interessiert nur der Zahlenwert $\{ G \}$), dann existieren zwar statistische Abweichungen, welche wir aber durch den Mittelwert gut reduzieren können. Liegen nun Messergebnisse $x_{1},x_{2},x_{3},\dots$ vor, berechnen wir das **arithmetische Mittel** über 

$$
\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}
$$

Je mehr Durchführungen wir vornehmen, desto näher kommen wir an den wahren Wert $x_{w}$. Wie genau wir nun an den wahren Wert sind, bestimmen wir über die empirische Standardabweichung $\sigma_{e}$, die mathematisch so beschrieben wird

$$
\sigma_{e}=\sqrt{ \frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2} }
$$
Wir nennen sie auch **Standardunsicherheit**. Über diese Terme lässt sich unser $G$ als $G=(\bar{x}\pm\sigma_{e})[G]$ darstellen.

## 2.5.3 Erweiterte Messunsicherheit
***

Nun geht es um Verteilungen. Unsere Messwerte *streuen* um unseren wahren Wert, wir wollen also wissen, **mit welcher Wahrscheinlichkeit** sie um den wahren Wert streuen.

![[Pasted image 20240821174218.png]]

Oben sehen wir ein **Histogramm**. Ein Histogramm ist ähnlich wie ein sehr dichtes Balkendiagramm. Wir wählen uns sogenannte **Bins**, das sind die Balken, mit einer Breite $\delta x_{i}$. Alle gemessenen Werte, die innerhalb dieser Breite liegen, werden für diesen Bin eingetragen. Die Menge aller Einträge sieht man dann auf der y-Achse. In unserem Fall ist das Histogramm auf $1$ **normiert**. Das heißt man teilt die Menge der Einträge pro Bin $\delta n_{i}$, durch die Gesamtanzahl $n$. Ihre Summe ist dann $1$. Führen wir nun $n \to \infty$ Messungen durch und schrumpfen $\delta x_{i}\to_{0}$, erhalten wir eine **kontinuierliche Verteilungsfunktion**.

Diese Verteilungsfunktion ist für physikalische Größen **normalverteilt**, d.h. sie entspricht der Funktion

$$
p(x)=\frac{1}{\sqrt{ 2\pi }\sigma}e^{-(x-\mu)^{2}/(2\sigma^{2})}
$$

Integriert man diese Funktion über ein beliebiges Intervall, erhalten wir die Wahrscheinlichkeit, dass ein zufällig gemessener Wert in diesem Intervall liegt. Dabei ist $\mu$ unser wahrer Wert, und $\sigma$ unsere Unsicherheit. Sehr praktisch.

Wir können $\mu$ über das Integral

$$
x_{w}=\mu=\int_{-\infty}^{\infty}x\cdot p(x)\text{d}x
$$

bestimmen. $\sigma^{2}$, die sogenannte **Varianz** ist dann

$$
V=\sigma^{2}=\int_{-\infty}^{\infty}(x-\mu)^{2}\cdot p(x)\text{d}x
$$

Wollen wir nun wissen, wie hoch die Wahrscheinlichkeit ist, dass ein Wert in einem Bereich $\delta$ um $x_{w}$ streut, integrieren wir von $x_{w}-\delta$ bis $x_{w}+\delta$. Wir nennen diese Wahrscheinlichkeit **Statistische Sicherheit**, oder auch **Konfidenz**. Das coole an der Gaußverteilung ist, dass für bestimmte Integrale immer der selbe Wert herauskommt. Möchte man eine Sicherheit von 95% haben, wissen wir, dass sich der Wert im Intervall von $\delta=1.96\sigma$ befinden wird. **Das ist immer so!**. Weitere Beispiele hier

| Intervall             | Wahrscheinlichkeit |
| --------------------- | ------------------ |
| $\|x-x_{w}\|<1\sigma$ | 68.26%             |
| $\|x-x_{w}\|<2\sigma$ | 95.45%             |
| $\|x-x_{w}\|<3\sigma$ | 99.73%             |

Wollen wir also mit einer Konfidenz von 95% unseren Messwert für $G$ angeben, schreiben wir

$$
G=(\bar{x}\pm 1.96\sigma_{e})[G]
$$

Damit wir einen Messwert als wahr anerkennen, muss meistens eine $5\sigma$ Konfidenz erreicht werden.

## 2.5.4 Messunsicherheit des Mittelwerts
***

Tatsächlich besitzt unser gemessener Mittelwert auch wieder eine Unsicherheit. Es existiert also eine Unsicherheit auf den Mittelwert, eine Unsicherheit auf die Unsicherheit, und einen Mittelwert, sowie Unsicherheit für unsere gesuchte Größe. Wir nennen diese Größe auch **Fehler auf den Mittelwert** welche wir über 

$$
\sigma_{m}=\frac{\sigma}{\sqrt{ n }}
$$

berechnen. $n$ ist dabei die Anzahl der Messungen. Um nun eine beste Schätzung des Zahlenwerts anzugeben, geben wir für $\{ G \}=\bar{x}\pm\sigma_{e}/\sqrt{ n }$ an. Je mehr Messungen wir anstellen, desto kleiner wird $\sigma_{m}$ und wir kommen näher an $x_{w}$. Die relative Unsicherheit ist dabei ein prozentuales Maß, und wird mit $\frac{\sigma_{m}}{\bar{x}}$. berechnet.