***

Bestimmt habt ihr schonmal diesen Ausdruck $\ket{\psi}$ gesehen, doch was bedeutet er genau? $\ket{\psi}$ ist ein **Vektor** in einem komplexen Vektorraum $\mathcal{H}$ und wird auch **ket-Vektor** genannt. Die ganzen Formalitäten, was einen komplexen Vektorraum ausmacht werden wir überspringen. Das ganze ist nur Konvention, es gilt jedoch $\ket{a}\leftrightarrow \vec{a}$, das bedeutet auch, dass wir ein Skalarprodukt bilden können, was $\mathcal{H}$ **unitär** macht. Dieses Skalarprodukt besitzt folgende Eigenschaften:

1. $\braket{a|a}\geq 0$; Das gleich entsteht *nur* wenn $\ket{a}=\ket{0}$ ist.
2. $\braket{ a |b  }=\braket{ b |a  }^{*}$
3. Wenn $\ket{b}=\lambda \ket{b_{1}}+\mu \ket{b_{2}}$; $\lambda,\mu \in \mathbb{C}$, dann gilt $\braket{ a |b  }=\lambda \braket{ a |b_{1}  }+\mu \braket{ a |b_{2}  }$

Für den dritten Fall gibt es natürlich auch Fälle, wo $\braket{ b |a  }$ auftreten kann, in diesem Fall können wir **nicht** einfach die Konstanten $\lambda,\mu$ herausziehen, sondern sie komplex konjugieren. Diese Eigenschaft nennt man **antilinear**, und das gilt nur, wenn dieses $\ket{b}$ als erstes auftaucht. Das Skalarprodukt ist also **antilinear im ersten Argument**. Der Beweis ist einfach:

$$
\begin{align}
\braket{ b |a  }&= \braket{ a |b  }^{*} \\
&=(\lambda \braket{ a |b_{1}  }+\mu \braket{ a |b_{2}  }  )^{*} \\
&=\lambda ^{*}\braket{ a |b_{1}  }+\mu ^{*}\braket{ a |b_{2}  }   
\end{align}
$$

Wie wir es auch aus der Schule kennen, können zwei Vektoren orthogonal aufeinander stehen, dies erkennt man, wenn $\braket{ a | b }=0$ ist - der Betrag lässt sich hier auch aus 1. einführen, denn daraus folgt $\braket{ a | a }\in \mathbb{R}$ und wir können somit die Norm $\lvert \lvert \ket{a} \rvert \rvert:= \sqrt{ \braket{ a | a } }$ definieren.

Wie man die Dimension von $\mathcal{H}$ findet, kennen wir auch aus HMII, einfach herausfinden, wie viele linear unabhängige Basisvektoren man basteln kann (leichter gesagt als getan). Zählt man dann $N$ Basisvektoren $\{ \ket{e_{1}},\ket{e_{2}},\dots,\ket{e_{N}} \}$, bei denen

$$
\braket{ e_{i} |e_{j}  }=\delta_{ij} 
$$

gilt, so bilden sie eine **Orthonormalbasis**. Man kann aus ihnen also jedes beliebige Element $\ket{a}$ bilden, gemäß

$$
\ket{a} =\sum_{j=1}^{N}a_{j} \ket{e_{j}} 
$$

Nutzt man also nun die die Antilinearität aus und will mit dieser Schreibweise $\braket{ a | b }$ bilden, so brauchen wir nur $a_{i}$ komplex konjugieren und die beiden Basisvektoren miteinander multiplizieren lassen:

$$
\braket{ a |b  }=\sum_{i,j=1}^{N} a_{i}^{*} b_{j} \braket{ e_{i} |e_{j}  }=\sum_{j=1}^{N}a_{j}^{*}b_{j}
$$

Bis jetzt haben wir ja auch $\bra{a}$ noch gar nicht beschrieben, was wir nun tun werden, indem wir lineare Funktionen $\varphi$ betrachten. Diese nehmen als Argument einen beliebigen ket-Vektor $\varphi(\ket{a}) \in \mathbb{C}, \ket{a}\in \mathcal{H}$ und schreiben diesem eine *eindeutige Zahl* zu. Das macht sie aber noch nicht linear, erst wenn 

$$
\varphi(\lambda \ket{a}+\mu \ket{b}  )=\lambda \varphi(\ket{a} )+\mu \varphi(\ket{b} )
$$

gilt, wobei $\lambda,\mu \in \mathbb{C},\,\ket{a},\ket{b}\in \mathcal{H}$ gilt. Ein einfaches Beispiel wäre $\varphi(\ket{a})=x+y+z$. Dies hat zur Folge, dass $\varphi$ unsere Basis $\{ \ket{e_{1}},\ket{e_{2}},\dots,\ket{e_{N}} \}$ eindeutig transformiert. Es ergibt sich also für $\varphi(\ket{e_{i}})=f_{i}, \, f_{i}\in \mathbb{C}$ das Resultat $\ket{a}=\sum_{i=1}^{N}a_{i}\ket{e_{i}}\implies \varphi(\ket{a})=\sum_{i=1}^{N}a_{i}f_{i}$, da eben diese Linearität gilt. Wir können ein weiteres Beispiel für $\varphi$ erzeugen, dabei betrachten wir jede Komponente eines Elements $\ket{a}\in \mathcal{H}$ ganz einfach gemäß

$$
\varphi^{\{ \ket{e_{i}} \} }_{j}(\ket{a} ):=a_{j}
$$

Diese Funktion ist offensichtlich linear für jedes $j=1,2,\dots,N$ auf $\mathcal{H}$, da es nur die $j$-ste Komponente projiziert. Der Supra Index gibt nur an, dass wir die oben genannte Basis verwenden. Da unsere Basis orthonormal ist, kann $\varphi$ nur einen Wert nicht Null ausgeben, wenn der $j$-ste Einheitsvektor als Argument angegeben wird. Es gilt also

$$
\varphi_{j}^{\{ \ket{e_{i}}  \}}(\ket{e_{k}} )=\delta_{jk}
$$

Diese Funktionen beschreiben wiederum einen Vektorraum, den **dualen Raum** $\mathcal{H}^{*}$, das haben wir in HMII bewiesen, sodass $\lambda \varphi+\mu \psi \in \mathbb{C}$ gilt, wenn $\psi$ eine weitere lineare Funktion ist und $\lambda,\mu$ unsere Konstanten bleiben. Streng genommen kann die obige Funktionen auch eine Basis bilden:

Wir gehen zurück zur linearen Funktion $\varphi(\ket{e_{j}})=f_{j}$. Den Wert $f_{j}$ kann man genauso über $\varphi_{j}^{\{ \ket{e_{i}} \}}$ bilden, wenn sie ein Basiselement annimmt, indem man sie einfach mit $f_{j}$ multipliziert. Und wie oben genannt "überlebt" einzig und allein $f_{j}$, sonst wird alles Null! D.h. auch die Funktion $\sum_{j=1}^{N} f_{j}\varphi^{\{ \ket{e_{i}} \}}_{j}$ beschreibt $\varphi$! Dies bedeutet wiederum, dass man $\varphi$ als Linearkombination darstellen kann und eine Basis bildet.

Es ist offensichtlich, dass wenn $\mathcal{H}$ $N$ Basiselemente besitzt, dann besitzt auch $\mathcal{H}^{*}$ $N$ Basiselemente, sie besitzen die gleiche Dimension. Bisher wissen wir nur, wie wenn man $\ket{a}$ kennt auf $\varphi$ zu schließen, also von $\mathcal{H}\to \mathcal{H}^{*}$, das geht jedoch auch andersrum. Stichwort; Skalarprodukt:

$$
\begin{align}
\braket{ e_{j} | a }&=\braket{ e_{j} | \left( \sum_{i=1}^{N}a_{i}\ket{e_{i}}  \right) } \\
&=\sum_{i=1}^{N}a_{i} \braket{ e_{j} |e_{i}  } \\
&=a_{j}
\end{align}
$$

Und siehe da, das sieht schon verdächtig ähnlich zu unserer Basis $\varphi^{\{ \ket{e_{i}} \}}_{j}$ aus. Somit hat unser $\varphi$ etwas mit dem Skalarprodukt zutun. Aus unserer anfänglichen Beobachtung, dass $\varphi(\ket{a})=\sum_{i=1}^{N}f_{i}a_{i}$ entspricht, liegt Nahe, dass sich aus beliebigen $\varphi=\sum_{j=1}^{N}f_{j}\varphi_{j}^{\{ \ket{e_{i}} \}}$ der Vektor $\ket{\varphi}=\sum_{j=1}^{N}f_{j}^{*}\ket{e_{j}}$ (Antilinearität im ersten Argument, Wichtig!!!) bilden lässt. Diese Bedingung erfüllt schlussendlich

$$
\varphi(\ket{a} )=\sum_{j=1}^{N}f_{j}a_{j}=\braket{ \varphi | a } 
$$

Dies bestätigt unsere Vermutung, wie $\bra{\varphi}$ Vektoren aufgebaut sind. Wir nennen sie **bra-Vektoren**. Jedes Element (egal ob in $\mathcal{H}$ oder $\mathcal{H}^{*}$) besitzt also ein Gegenstück im jeweils anderen Raum, die im Skalarprodukt $\braket{ \varphi | a }$ "Bracket" ergeben - das englisch Wort für Klammer. Sehr kreativ... Wir können also nun unserer Funktion $\varphi^{\{ \ket{e_{i}} \}}_{j}$ einen bra-Vektor zuschreiben, der eben $\bra{e_{j}}$ ist. Die Menge $\{ \bra{e_{1}},\bra{e_{2}},\dots,\bra{e_{N}} \}$ bilden eine Basis in $\mathcal{H}^{*}$, dies ist einfache Konvention. Wir sehen also sofort, dass die beiden Vektorräume verknüpft sind. Das Element aus dem Dualraum entspricht den bra-Vektoren, das Element aus dem Vektorraum $\mathcal{H}$ den ket-Vektoren. Es gilt einfach:

$$
\ket{a}=\sum_i \lambda_{i}\ket{a}\leftrightarrow \bra{a}=\sum_{i}\lambda_{i}^{*}\bra{a} 
$$

Wir brauchen nicht nur Funktionen betrachten, öfters nutzt man auch lineare Abbildungen $\mathbf{A}$. Diese sollen von $\mathcal{H}\to \mathcal{H}$ abbilden. $\mathbf{A}$ ist eine **lineare Selbstabbildung**, wenn

$$
\mathbf{A}(\lambda \ket{a}+\mu \ket{b}  )=\lambda \mathbf{A}\ket{a}+\mu \mathbf{A}\ket{b}  
$$


gilt. Der Operator $\mathbf{A}$ soll auch die Eigenschaft erfüllen, dass in Bezug auf einer Basis $\{ \ket{g_{1}},\ket{g_{2}},\dots,\ket{g_{N}} \}$ (diese muss nicht orthonormal sein) auch die Elemente $\mathbf{A}\ket{g_{i}}$ für $i=1,2,\dots,N$ ein ket-Vektor ergeben, er ist also vollständig festgelegt. Daraus lassen sich Koeffizienten ergeben, Koeffizienten sind komplexe Werte, die aus einer linearen Funktion stammen. Oder über der linearen Abbildung über $\braket{ g_{i}|\mathbf{A} |g_{j} }$, wobei die Koeffizienten ihre Matrixelemente sind.

