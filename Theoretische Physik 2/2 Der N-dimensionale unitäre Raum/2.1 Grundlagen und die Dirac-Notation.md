***

Bestimmt habt ihr schonmal diesen Ausdruck $\ket{\psi}$ gesehen, doch was bedeutet er genau? $\ket{\psi}$ ist ein **Vektor** in einem komplexen Vektorraum $\mathcal{H}$ und wird auch **ket-Vektor** genannt. Die ganzen Formalitäten, was einen komplexen Vektorraum ausmacht werden wir überspringen. Das ganze ist nur Konvention, es gilt jedoch $\ket{a}\leftrightarrow \vec{a}$, das bedeutet auch, dass wir ein Skalarprodukt bilden können, was $\mathcal{H}$ **unitär** macht. Dieses Skalarprodukt besitzt folgende Eigenschaften:

1. $\braket{a|a}\geq 0$; Das gleich entsteht *nur* wenn $\ket{a}=\ket{0}$ ist.
2. $\braket{ a |b  }=\braket{ b |a  }^{*}$
3. Wenn $\ket{b}=\lambda \ket{b_{1}}+\mu \ket{b_{2}}$; $\lambda,\mu \in \mathbb{C}$, dann gilt $\braket{ a |b  }=\lambda \braket{ a |b_{1}  }+\mu \braket{ a |b_{2}  }$

Für den dritten Fall gibt es natürlich auch Fälle, wo $\braket{ b |a  }$ auftreten kann, in diesem Fall können wir **nicht** einfach die Konstanten $\lambda,\mu$ herausziehen, sondern sie komplex konjugieren. Diese Eigenschaft nennt man **antilinear**, und das gilt nur, wenn dieses $\ket{b}$ als erstes auftaucht. Das Skalarprodukt ist also **antilinear im ersten Argument**. Der Beweis ist einfach:

$$
\begin{align}
\braket{ b |a  }&= \braket{ a |b  }^{*} \\
&=(\lambda \braket{ a |b_{1}  }+\mu \braket{ a |b_{2}  }  )^{*} \\
&=\lambda ^{*}\braket{ a |b_{1}  }+\mu ^{*}\braket{ a |b_{2}  }   
\end{align}
$$

Wie wir es auch aus der Schule kennen, können zwei Vektoren orthogonal aufeinander stehen, dies erkennt man, wenn $\braket{ a | b }=0$ ist - der Betrag lässt sich hier auch aus 1. einführen, denn daraus folgt $\braket{ a | a }\in \mathbb{R}$ und wir können somit die Norm $\lvert \lvert \ket{a} \rvert \rvert:= \sqrt{ \braket{ a | a } }$ definieren.

Wie man die Dimension von $\mathcal{H}$ findet, kennen wir auch aus HMII, einfach herausfinden, wie viele linear unabhängige Basisvektoren man basteln kann (leichter gesagt als getan). Zählt man dann $N$ Basisvektoren $\{ \ket{e_{1}},\ket{e_{2}},\dots,\ket{e_{N}} \}$, bei denen

$$
\braket{ e_{i} |e_{j}  }=\delta_{ij} 
$$

gilt, so bilden sie eine **Orthonormalbasis**. Man kann aus ihnen also jedes beliebige Element $\ket{a}$ bilden, gemäß

$$
\ket{a} =\sum_{j=1}^{N}a_{j} \ket{e_{j}} 
$$

Nutzt man also nun die die Antilinearität aus und will mit dieser Schreibweise $\braket{ a | b }$ bilden, so brauchen wir nur $a_{i}$ komplex konjugieren und die beiden Basisvektoren miteinander multiplizieren lassen:

$$
\braket{ a |b  }=\sum_{i,j=1}^{N} a_{i}^{*} b_{j} \braket{ e_{i} |e_{j}  }=\sum_{j=1}^{N}a_{j}^{*}b_{j}
$$

Bis jetzt haben wir ja auch $\bra{a}$ noch gar nicht beschrieben, was wir nun tun werden, indem wir lineare Funktionen $\varphi$ betrachten. Diese nehmen als Argument einen beliebigen ket-Vektor $\varphi(\ket{a}) \in \mathbb{C}, \ket{a}\in \mathcal{H}$ und schreiben diesem eine *eindeutige Zahl* zu. Das macht sie aber noch nicht linear, erst wenn 

$$
\varphi(\lambda \ket{a}+\mu \ket{b}  )=\lambda \varphi(\ket{a} )+\mu \varphi(\ket{b} )
$$

gilt, wobei $\lambda,\mu \in \mathbb{C},\,\ket{a},\ket{b}\in \mathcal{H}$ gilt. Ein einfaches Beispiel wäre $\varphi(\ket{a})=x+y+z$. Dies hat zur Folge, dass $\varphi$ unsere Basis $\{ \ket{e_{1}},\ket{e_{2}},\dots,\ket{e_{N}} \}$ eindeutig transformiert. Es ergibt sich also für $\varphi(\ket{e_{i}})=f_{i}, \, f_{i}\in \mathbb{C}$ das Resultat $\ket{a}=\sum_{i=1}^{N}a_{i}\ket{e_{i}}\implies \varphi(\ket{a})=\sum_{i=1}^{N}a_{i}f_{i}$, da eben diese Linearität gilt. Wir können ein weiteres Beispiel für $\varphi$ erzeugen, dabei betrachten wir jede Komponente eines Elements $\ket{a}\in \mathcal{H}$ ganz einfach gemäß

$$
\varphi^{\{ \ket{e_{i}} \} }_{j}(\ket{a} ):=a_{j}
$$

Diese Funktion ist offensichtlich linear für jedes $j=1,2,\dots,N$ auf $\mathcal{H}$, da es nur die $j$-ste Komponente projiziert. Der Supra Index gibt nur an, dass wir die oben genannte Basis verwenden. Da unsere Basis orthonormal ist, kann $\varphi$ nur einen Wert nicht Null ausgeben, wenn der $j$-ste Einheitsvektor als Argument angegeben wird. Es gilt also

$$
\varphi_{j}^{\{ \ket{e_{i}}  \}}(\ket{e_{k}} )=\delta_{jk}
$$

Diese Funktionen beschreiben wiederum einen Vektorraum, den **dualen Raum** $\mathcal{H}^{*}$, das haben wir in HMII bewiesen, sodass $\lambda \varphi+\mu \psi \in \mathbb{C}$ gilt, wenn $\psi$ eine weitere lineare Funktion ist und $\lambda,\mu$ unsere Konstanten bleiben. Streng genommen kann die obige Funktionen auch eine Basis bilden:

Wir gehen zurück zur linearen Funktion $\varphi(\ket{e_{j}})=f_{j}$. Den Wert $f_{j}$ kann man genauso über $\varphi_{j}^{\{ \ket{e_{i}} \}}$ bilden, wenn sie ein Basiselement annimmt, indem man sie einfach mit $f_{j}$ multipliziert. Und wie oben genannt "überlebt" einzig und allein $f_{j}$, sonst wird alles Null! D.h. auch die Funktion $\sum_{j=1}^{N} f_{j}\varphi^{\{ \ket{e_{i}} \}}_{j}$ beschreibt $\varphi$! Dies bedeutet wiederum, dass man $\varphi$ als Linearkombination darstellen kann und eine Basis bildet.

Es ist offensichtlich, dass wenn $\mathcal{H}$ $N$ Basiselemente besitzt, dann besitzt auch $\mathcal{H}^{*}$ $N$ Basiselemente, sie besitzen die gleiche Dimension. Bisher wissen wir nur, wie wenn man $\ket{a}$ kennt auf $\varphi$ zu schließen, also von $\mathcal{H}\to \mathcal{H}^{*}$, das geht jedoch auch andersrum. Stichwort; Skalarprodukt:

$$
\begin{align}
\braket{ e_{j} | a }&=\braket{ e_{j} | \left( \sum_{i=1}^{N}a_{i}\ket{e_{i}}  \right) } \\
&=\sum_{i=1}^{N}a_{i} \braket{ e_{j} |e_{i}  } \\
&=a_{j}
\end{align}
$$

^963734

Und siehe da, das sieht schon verdächtig ähnlich zu unserer Basis $\varphi^{\{ \ket{e_{i}} \}}_{j}$ aus. Somit hat unser $\varphi$ etwas mit dem Skalarprodukt zutun. Aus unserer anfänglichen Beobachtung, dass $\varphi(\ket{a})=\sum_{i=1}^{N}f_{i}a_{i}$ entspricht, liegt Nahe, dass sich aus beliebigen $\varphi=\sum_{j=1}^{N}f_{j}\varphi_{j}^{\{ \ket{e_{i}} \}}$ der Vektor $\ket{\varphi}=\sum_{j=1}^{N}f_{j}^{*}\ket{e_{j}}$ (Antilinearität im ersten Argument, Wichtig!!!) bilden lässt. Diese Bedingung erfüllt schlussendlich

$$
\varphi(\ket{a} )=\sum_{j=1}^{N}f_{j}a_{j}=\braket{ \varphi | a } 
$$

Dies bestätigt unsere Vermutung, wie $\bra{\varphi}$ Vektoren aufgebaut sind. Wir nennen sie **bra-Vektoren**. Jedes Element (egal ob in $\mathcal{H}$ oder $\mathcal{H}^{*}$) besitzt also ein Gegenstück im jeweils anderen Raum, die im Skalarprodukt $\braket{ \varphi | a }$ "Bracket" ergeben - das englisch Wort für Klammer. Sehr kreativ... Wir können also nun unserer Funktion $\varphi^{\{ \ket{e_{i}} \}}_{j}$ einen bra-Vektor zuschreiben, der eben $\bra{e_{j}}$ ist. Die Menge $\{ \bra{e_{1}},\bra{e_{2}},\dots,\bra{e_{N}} \}$ bilden eine Basis in $\mathcal{H}^{*}$, dies ist einfache Konvention. Wir sehen also sofort, dass die beiden Vektorräume verknüpft sind. Das Element aus dem Dualraum entspricht den bra-Vektoren, das Element aus dem Vektorraum $\mathcal{H}$ den ket-Vektoren. Es gilt einfach:

$$
\ket{a}=\sum_i \lambda_{i}\ket{a}\leftrightarrow \bra{a}=\sum_{i}\lambda_{i}^{*}\bra{a} 
$$

Wir brauchen nicht nur Funktionen betrachten, öfters nutzt man auch lineare Abbildungen $\mathbf{A}$. Diese sollen von $\mathcal{H}\to \mathcal{H}$ abbilden. $\mathbf{A}$ ist eine **lineare Selbstabbildung**, wenn

$$
\mathbf{A}(\lambda \ket{a}+\mu \ket{b}  )=\lambda \mathbf{A}\ket{a}+\mu \mathbf{A}\ket{b}  
$$


gilt. Der Operator $\mathbf{A}$ soll auch die Eigenschaft erfüllen, dass in Bezug auf einer Basis $\{ \ket{g_{1}},\ket{g_{2}},\dots,\ket{g_{N}} \}$ (diese muss nicht orthonormal sein) auch die Elemente $\mathbf{A}\ket{g_{i}}$ für $i=1,2,\dots,N$ ein ket-Vektor ergeben, er ist also vollständig festgelegt. Daraus lassen sich Koeffizienten ergeben, Koeffizienten sind komplexe Werte, die aus einer linearen Funktion stammen. Oder über der linearen Abbildung über $\braket{ g_{i}|\mathbf{A} |g_{j} }$, wobei die Koeffizienten ihre Matrixelemente sind.

Es gilt hierbei, wenn wir bspw. $\braket{ a | \mathbf{A}|b }$ bilden, dass der ket $\ket{b}$ linear ist. Analog, wenn wir einen bra auf $\mathbf{A}$ anwenden, so erhalten wir einen Vektor $\bra{c}$ im $\mathcal{H}^{*}$ gemäß $\bra{c}:= \bra{a}\mathbf{A}$. Wir brauchen nun ein paar weitere Rechenregeln, um herauszufinden was der zugehörige bra Vektor $\bra{c}\in \mathcal{H}^{*}$ zum ket Vektor $\ket{c}\in \mathcal{H}$ ist. Ähnlich wie wir es mit reellen Matrizen $\mathbf{M}$ im $\mathbb{R}^{n\times n}$ machen, so können wir diese **transponieren** $\mathbf{M}^{\text{T}}$, wir tauschen also jede Spalte mit ihrer Zeile. Wenn nun $\vec{a}$ und $\vec{b}$ im $\mathbb{R}^{n}$ sind, dann erhalten wir durch eine Art Skalarprodukt das Skalar $\vec{a}\cdot \mathbf{M}\cdot \vec{b}$ ($\vec{a}$ ist ein Zeilenvektor, nur angemerkt). Transponieren wir nun diesen Ausdruck, so erhalten wir $\vec{b}\cdot \mathbf{M}^{\text{T}}\cdot \vec{a}$ (Der erste Zeilenvektor wird zum Spaltenvektor, d.h. wir müssen ihn auf die andere Seite bringen, sonst erhalten wir einen Tensor), wobei wenn man ein Skalar transponiert, denselben Wert erhält (Tensor 0. Stufe). Diese beiden Ausdrücke decken sich.

Wenn wir ins komplexe erweitern, so müssen wir etwas aufpassen. Das Transponieren geht mit einem **konjugieren** einher, wir nennen dies dann auch **hermitesch-transponieren** und schreiben statt T ein H. An sich ändert sich aber an dem Ausdruck nichts, abgesehen, dass wir da ein H stehen haben. In der Quantenmechanik werden wir auch statt dem H den sogenannten **Dolch** $\dagger$ nutzen, um hermitesch-Transponieren darzustellen. Das man diese Identität aus dem konjugieren $^{*}$ herleiten kann, lässt sich leicht zeigen:

$$
(\vec{a}^{*}\cdot \mathbf{M}\cdot \vec{b})^{*}=(\vec{a}\cdot \mathbf{M}^{*}\cdot \vec{b}^{*})^{\text{adjungiert}}=\vec{b}^{*}\cdot \mathbf{M}^{*}\cdot \vec{a}
$$

Wir haben hier ganz lax "adjungiert" hingeschrieben, was noch bedeutet, dass wir das ganze transponieren müssen. Übertragen wir diese Gedanken auf die Quantenmechanik, so können wir das Matrixprodukt ebenso als $\braket{ a | \mathbf{A}|b }$ darstellen, und wenn wir es konjugieren, so kommen wir auf 

$$
\braket{ b | \mathbf{A}^{\dagger}|a }:=\braket{ a | \mathbf{A}|b } ^{*} 
$$

^6f1aa6

Für alle $\ket{a},\ket{b}\in \mathcal{H}$. Ähnlich wie beim Transponieren/Konjugieren gilt, dass wenn wir den Operator zweimal anwenden unseren ursprünglichen Ausdruck erhalten. Das bedeutet:

$$
\braket{ b | \mathbf{A}|a }= \braket{ a | \mathbf{A}^{\dagger}|b}^{*}=\braket{ b | (\mathbf{A}^{\dagger})^{\dagger}|a }   
$$

Und man liest $(\mathbf{A}^{\dagger})^{\dagger}=\mathbf{A}$ ab. Nun erinnern wir uns, dass man einen bra Vektor $\bra{c}$ als eine lineare Abbildung eines anderen bra Vektors $\bra{a}$ multipliziert mit einer Matrix $\mathbf{A}$ darstellen kann, also $\bra{c}=\bra{a}\mathbf{A}$. Dies können wir nun ausnutzen:

$$
\bra{b}\mathbf{A}^{\dagger}\ket{a} =(\underbrace{ \bra{a}\mathbf{A} }_{ =\bra{c}  }\ket{b})^{*}=\braket{ c |b  }^{*}=\braket{ b | c } 
$$

Und nun erkennen wir etwas unglaubliches, wenn wir den ersten und zweiten Term vergleichen! Wir erkennen nämlich auch, dass $\ket{c}=\mathbf{A}^{\dagger}\ket{a}$ ist! Also:

$$
\bra{c}=\bra{a}\mathbf{A}\leftrightarrow \ket{c}=\mathbf{A}^{\dagger}\ket{a}    
$$

Oder wenn wir uns den bra Vektor $\bra{b}$ anschauen, finden wir den zweiten Zusammenhang, da wir ja vorausgesetzt haben, dass man einen ket Vektor als $\ket{c}=\mathbf{A}\ket{a}$ schreiben kann aus der gleichen Gleichung (Nur in dem man eben $\mathbf{A}\ket{b}$ einsetzt)

$$
\bra{c}=\bra{a}\mathbf{A}^{\dagger}\leftrightarrow  \ket{c}=\mathbf{A}\ket{a}  
$$

Man kann das ganze auch in Worte fasse: Der adjungierte Operator, das kann also $\mathbf{A}^{\dagger}$ oder $\mathbf{A}$ sein (es ist ja nur eine anders geschrieben Matrix, die Konvention ist egal), wirkt nach links wie der Operator $\mathbf{A}$, oder $\mathbf{A}^{\dagger}$, selbst nach rechts. Wir brauchen noch eine Definition, und die erhalten wir durch ein Beispiel: Wir wählen einen Operator $\mathbf{A}$ der jeden ket Vektor $\ket{b}\in \mathcal{H}$ auf $\mathbf{A}\ket{b}=\alpha \ket{b}$ abbildet (im reellen Fall wäre das eine Streckung, hier ist aber $\alpha \in \mathbb{C}$). Wir nutzen wieder [[#^6f1aa6]] 

$$
\bra{b}\mathbf{A}^{\dagger}\ket{a}=\bra{a}\mathbf{A}\ket{b}^{*}=(\alpha \braket{ a |b  } )^{*}=\alpha ^{*}\braket{ b |a  }=\bra{b} (\alpha ^{*}\ket{a} )  
$$

Und nun erkennen wir wieder durch Abgleichen der Terme $\mathbf{A}^{\dagger}\ket{a}=\alpha ^{*}\ket{a}$. Den adjungierten Operator legen wir aber vorerst zur Seite.

Im konkreten Rechenfall nutzen wir für jeden Vektor $\ket{a}$ eine geeignete Orthonormalbasis $\{ \ket{e_{1}},\ket{e_{2}},\dots,\ket{e_{N}} \}$ , wodurch wir auch $\ket{a}$ als Linearkombination darstellen können

$$
\ket{a}=\sum_{j=1}^{N} a_{j}\ket{e_{j}}  
$$

$a_{j}\in \mathbb{C}$ natürlich, da es etwas nervig ist immer $\ket{e_{j}}$ zu schreiben, und jeder weiß was der Ausdruck je nach $j$ aussagt, schreiben wir ab jetzt $\ket{e_{j}}:=\ket{j}$. Aus [[#^963734]] wissen wir, dass $a_{j}=\braket{ j | a }$ gilt, dies setzen wir also oben ein, und nutzen Kommutativität aus:

$$
\ket{a}=\sum_{j=1}^{N} \braket{ j | a } \ket{j}=\sum_{j=1}^{N} \ket{j} \braket{ j | a }  
$$

Achten wir nun auf den letzten Term, so steht dort systematisch, dass "etwas" ($\sum_{j=1}^{N} \ket{j}\bra{j}$) mal den Vektor, wieder den Vektor ergibt. Das kann nur sein, wenn der Summenausdruck **die Identität** $\mathbf{1}$ ist. Also:

$$
\mathbf{1}=\sum_{j=1}^{N} \ket{j}\bra{j}  
$$

^7754bd

Diese Gleichung wird **Vollständigkeitsrelation** genannt, und wird für die unendlich dimensionalen Räume wichtig, aber ist auch jetzt sehr wichtig. Da jeder einzelne Term der Summe $\ket{j}\bra{j}$ seinen Index herausprojiziert, nennt man den Term auch **Projektionsoperator** $\mathbf{P}_{j}:=\ket{j}\bra{j}$. Angewandt schreibt es sich also:

$$
\mathbf{P}_{j}\ket{a} =\ket{j} \braket{ j | a } =a_{j}\ket{j} 
$$

Eine andere Schreibweise ist also:

$$
\mathbf{1}=\sum_{j=1}^{N}\mathbf{P}_{j}
$$

Multiplizieren wir zwei dieser Operatoren, so erhalten wir eine Matrixmultiplikation mit nur einem jeweils von $0$ verschiedenen Element. Es gilt also

$$
\mathbf{P}_{i}\mathbf{P}_{j}=\ket{i} \braket{ i | j } \bra{j} =\delta_{ij}\mathbf{P}_{i}
$$

Wir benutzen nochmal [[#^6f1aa6]] Um das Adjungierte $\mathbf{P}_{j}^{\dagger}$ zu finden:

$$
\begin{align}
\bra{b} \mathbf{P}_{j}^{\dagger} \ket{a} &=\bra{a} \mathbf{P}_{j}\ket{b}^{*}=(\braket{ a | j } \braket{ j | b } )^{*}=\braket{ a | j } ^{*}\braket{ j | b } ^{*}=\braket{ j | a } \braket{ b | j }  \\
&= \braket{ b | j } \braket{ j | b } =\bra{b} \mathbf{P}_{j}\ket{a} 
\end{align}
$$

das heißt $\mathbf{P}_{j}^{\dagger}=\mathbf{P}_{j}$! In diesem Fall nennt man den Operator **selbstadjungiert**. Diese sind von zentraler Bedeutung.

Die Bedeutsamkeit sehen wir aus der Vollständigkeitsrelation [[#^7754bd]], denn ein Operator $\mathbf{A}$ wird durch seine Elemente festgelegt. Diese wenden wir auf eine Orthonormalbasis, und da $\mathbf{A}\mathbf{1}=\mathbf{A}$ bzw. $\mathbf{1}\mathbf{A}\mathbf{1}=\mathbf{A}$ ist, lassen sich die Matrixelemente auch als Bracket mit den jeweiligen Basisvektoren notieren. Zuerst:

$$
\mathbf{A}=\mathbf{1}\mathbf{A}\mathbf{1}=\sum_{i,j=1}^{N}\ket{i} \bra{i} \mathbf{A}\ket{j} \bra{j} 
$$

der mittlere Term projiziert also das Element $A_{ij}:=\bra{i}\mathbf{A}\ket{j}$ heraus, und die zwei äußeren bras und kets reihen dieses Element in die richtige Reihe und Spalte ein. Wir können also hinsichtlich dieser Basis die Matrix auch als

$$
\mathbf{A}\dot{=}
\begin{pmatrix}
\bra{1} \mathbf{A}\ket{1} & \bra{1} \mathbf{A}\ket{2} & \dots & \bra{1} \mathbf{A}\ket{N} \\
\dots & \dots & \dots & \dots \\
. & . & . & . \\
. & . & . & . \\
. & . & . & .  \\
\dots & \dots & \dots & \dots \\
\bra{N}\mathbf{A}\ket{1} & \bra{N} \mathbf{A} \ket{2} & \dots & \bra{N} \mathbf{A} \ket{N}       
\end{pmatrix}
$$

schreiben. $\dot{=}$ bedeutet "wird dargestellt durch". Wir betrachten ein Beispiel, um zu zeigen, dass diese Matrix wirklich von der Basis abhängt. Wir nutzen dafür $\mathbf{P}_{1}=\ket{1} \bra{1}$, nutzen wir die Basis $\{ \ket{j} \}$, sieht $\mathbf{P}_{1}$

$$
\mathbf{P}_{1}\dot{=}
\begin{pmatrix}
1 & 0 & \dots & 0 \\
0 & 0 & \dots & 0 \\
\dots & \dots & \dots & \dots \\
. & . & . & . \\
. & . & . & . \\
. & . & . & . \\
\dots & \dots & \dots & \dots \\
0 & 0 & \dots & 0
\end{pmatrix}
$$

Da eben nur $\braket{ 1 | 1 }\braket{ 1 | 1 }=\delta_{11}\delta_{11}$ entspricht und damit $1$ ist. Wechseln wir aber die Orthonormalbasis zu $\{ \ket{\tilde{j}} \}$, dann erhalten wir

$$
\mathbf{P}_{1}\dot{=}
\begin{pmatrix}
\braket{ \tilde{1} | 1 } \braket{ 1 | \tilde{1} }  & \braket{ \tilde{1} | 1 } \braket{ 1 | \tilde{2} }  & \dots & \braket{ \tilde{1} | 1 } \braket{ 1 | \tilde{N} } \\
\braket{ \tilde{2} | 1 } \braket{ 1 | \tilde{1} }  & \braket{ \tilde{2} | 1 } \braket{ 1 | \tilde{2} }  & \dots & \braket{ \tilde{2} | 1 } \braket{ 1 | \tilde{N} } \\
\dots & \dots & \dots & \dots \\
. & . & . & . \\
. & . & . & . \\
. & . & . & . \\
\dots & \dots & \dots & \dots \\
\braket{ \tilde{N} | 1 } \braket{ 1 | \tilde{1} }  & \braket{ \tilde{N} | 1 } \braket{ 1 | \tilde{2} }  & \dots & \braket{ \tilde{N} | 1 } \braket{ 1 | \tilde{N} }
\end{pmatrix}
$$

