***

Wir verlasen erstmal die Dirac Notation und machen mit Kommutatoren weiter. Kommutatoren sind Operatoren, wie Matrizen, die durch

$$
[\mathbf{A},\mathbf{B}]:=\mathbf{AB}-\mathbf{BA}
$$

definiert sind. D.h. die Operatoren selbst bilden einen Operator. Dieses Konzept verallgemeinern wir , indem wir eine Funktion $f$ betrachten, die von einer komplexen Variable $z$ abhängt. Um uns auch auf eine allgemeine Funktion zu "beschränken", nutzen wir die Potenzreihendarstellung

$$
f(z)=\sum_{n=0}^{\infty}a_{n}z^{n}
$$

Konvergenzradien und unstetige Funktionen, die nicht entwickelbar sind ignorieren wir. Ähnlich wie beim Matrixexponential kann auch diese Funktion einen Operator $\mathbf{A}$ des $N$-dimensionalen unitären Raum als Variable annehmen, die Funktion $f(\mathbf{A})$ sieht dann so aus:

$$
f(\mathbf{A}):=\sum_{n=0}^{N}a_{n}\mathbf{A}^{n}
$$

Es gilt $\mathbf{A}^{0}=\mathbf{1}$ und $\mathbf{A}^{n}$ die $n$-fache Matrixmultiplikation mit dem Operator selbst. Ohne Beschränkung der Allgemeinheit gelten die bisher hergeleiteten Potenzreihen bekannter Funktionen bereits. Das Operatorexponential ist also wie gewohnt

$$
f(\mathbf{A})=\sum_{n=0}^{\infty} \frac{\mathbf{A}^{n}}{n!}=\exp(\mathbf{A})=e^{\mathbf{A}}
$$

Hier ist natürlich zu beachten, dass im skalaren Fall die $z$ kommutieren, Operatoren kommutieren allgemein aber nicht. Aufpassen. Diese Funktion lässt sich auch ableiten, jedoch nicht nach $\mathbf{A}$ selbst, sondern nur im Fall, dass $\mathbf{A}$ von einem Parameter $p$ abhängt. Die Ableitung sieht dann aus wie unser gewohnte Differenzenquotient

$$
\frac{\text{d}\mathbf{A}(p)}{\text{d}p}:=\lim_{ \Delta p \to 0 } \frac{\mathbf{A}(p+\Delta p)-\mathbf{A}(p)}{\Delta p} 
$$

Warum habe ich erwähnt, dass wir berücksichtigen müssen, dass Operatoren allgemein nicht Kommutieren? Nutzen wir ein Beispiel, ein sehr wichtiges sogar, nämlich $\mathbf{A}(p):=e^{p\mathbf{B}}$. Über die Kettenregel (bzw. der Potenzreihendarstellung) können wir einen wichtigen Zusammenhang finden:

$$
\frac{\text{d}\mathbf{A}(p)}{\text{d}p}=\mathbf{B}e^{p\mathbf{B}}=e^{p\mathbf{B}}\mathbf{B=\mathbf{B}}\mathbf{A}(p)=\mathbf{A}(p)\mathbf{B}
$$

Wir *dürfen* hier kommutieren, da $\exp(p\mathbf{B})$ nur Potenzen von $\mathbf{B}$ besitzt, und $\mathbf{B}$ offensichtlich mit sich selbst kommutiert. 

Als nächstes wollen wir herausfinden, ob wenn es zwei Operatoren $\mathbf{A},\mathbf{B}$ gibt, und diese selbstadjungiert, als auch kommutierend sind (also $[\mathbf{A},\mathbf{B}]=0$), diese auch gemeinsame Eigenvektoren besitzen. Dabei betrachten wir den Fall, wo mindestens eine Matrix nicht entartet ist, wir wählen dafür einfach $\mathbf{A}$. Diese Matrix soll also zu jedem Eigenwert $a_{j}$ einen eindeutigen Eigenvektor $\ket{a_{j}}$ besitzen. Wir nutzen

$$
[\mathbf{A},\mathbf{B}]=0 \implies \mathbf{AB}=\mathbf{BA}
$$

und multiplizieren beide Seiten mit $\ket{a_{j}}$:

$$
\mathbf{AB}\ket{a_{j}}=\mathbf{BA}\ket{a_{j}}=a_{j}\mathbf{B}\ket{a_{j}}  
$$

Interessanterweise können wir nun $\mathbf{B}\ket{a_{j}}$ als einen weiteren Vektor auffassen, der mit $\mathbf{A}$ multipliziert wird, und wir erkennen, dass dieser den Eigenwert, mal jenen Vektor entspricht! D.h. $\mathbf{B}\ket{a_{j}}$ ist ein Eigenvektor zu $\mathbf{A}$ zum Eigenwert $a_{j}$. Des Weiteren: Der Eigenvektor ist aber eindeutig festgelegt, dass bedeutet, dass nach der Multiplikation mit $\mathbf{B}$ und Multiplikation mit $a_{j}$ nur ein neuer Vorfaktor entsteht, es gilt also:

$$
\mathbf{B}\ket{a_{j}}=b_{j}\ket{a_{j}}  
$$

Aber das heißt doch, dass $\ket{a_{j}}$ auch ein Eigenvektor zu $\mathbf{B}$ ist! Darum haben wir auch geschickt $b_{j}$ als Vorfaktor gewählt, weil dieser Eigenwert zu $\mathbf{B}$ ist. Und im nicht entarteten Fall bilden die $\ket{a_{j}}$ einen Eigenraum, der die Matrizen in dieser Basis eindeutig festlegt:

$$
\begin{align}
\mathbf{A}&=\sum_{j=1}^{N}a_{j} \ket{a_{j}} \bra{a_{j}} \\
\mathbf{B} & =\sum_{j=1}^{N}b_j \ket{a_{j}} \bra{a_{j}}  \\
\mathbf{1}&=\sum_{j=1}^{N} \ket{a_{j}} \bra{a_{j}} 
\end{align}
$$

Nun müssen wir aber auch den anderen Fall betrachten. Sagen wir beide Operatoren besitzen entartete Eigenwerte, um auch zu wissen, wie "stark" ein Eigenwert $a_{j}$ (Wir betrachten erstmal nur $\mathbf{A}$) entartet, geben wir ihm einen **Entartungsgrad** $M_{j}\in \mathbb{N}$, es existieren also $n<N$ verschiedene Eigenwerte $a_{j}$ und jeder besitzt einen Entartungsgrad $M_{j}$ mit $j=1,2,\dots,n$. Ähnlich wie wir es in HMII mit Hauptvektoren zu einem Eigenwert $\lambda$ gemacht haben, in dem die geometrische Vielfachheit ungleich der algebraischen Vielfachheit ist, kann dieser Eigenwert einen Unterraum $\mathcal{H}_{j}$ zum Eigenwert $a_{j}$ aufspannen. Dieser besitzt dann $M_{j}$ Dimensionen (z.B. Entartungsgrad $M_{1}=3$, dann kann $a_{1}$ drei unterschiedliche Eigenvektoren erzeugen). Das bedeutet wiederum, dass wir auf eine Art Jordan-Block stoßen werden, wenn wir uns die Spektraldarstellung [[2.1 Grundlagen und die Dirac-Notation#^a789c9]] anschauen. Wir müssen also bei unserem Eigenvektor $\ket{a_{j}}$ erkennbar machen, der *wie vielte Eigenvektor er ist*. Und da ist der Physiker faul und schreibt einfach $\ket{a_{j},m_{j}}$. WICHTIG! $m_{j}$ ist ein eigener Parameter und läuft von $m_{j}=1,2,\dots,M_{j}$! 

Die Spektraldarstellung im entarteten Fall sieht also so aus:

$$
\mathbf{A}=\sum_{j=1}^{n}\sum_{m_{j}=1}^{M_{j}} a_{j} \ket{a_{j},m_{j}} \bra{a_{j},m_{j}} 
$$

Dies ist natürlich auch eine Diagonalmatrix, diese sieht aber so aus:

$$
\mathbf{A}\dot{=}
\begin{pmatrix}
a_{1} & 0 & 0 & \dots & \dots & \dots & \dots & \dots & 0 \\
0 & a_{1} & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
\dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & a_{1} & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & a_{2} & \dots & \dots & \dots & \dots \\
. & . & . & . & . & . & . & . & . \\
. & . & . & . & . & . & . & . & . \\
. & . & . & . & . & . & . & . & . \\
0 & 0 & 0 & \dots & \dots & \dots & \dots & \dots & a_{n}
\end{pmatrix}
$$

Nun gehen wir analog vor, um herauszufinden, wie $\ket{a_{j},m_{j}}$ auf $\mathbf{B}$ wirkt:

$$
\mathbf{AB}\ket{a_{j},m_{j}} =\mathbf{BA}\ket{a_{j},m_{j}}=a_{j}\mathbf{B}\ket{a_{j},m_{j}} 
$$

$\mathbf{B}\ket{a_{j},m_{j}}$ ist also auch Element von $\mathcal{H}_{j}$ zu $a_{j}$, für unterschiedliche Eigenwerten gilt wiederum, dass z.B. $\ket{a_{k},m_{k}}$ orthogonal auf $\mathbf{B}\ket{a_{j},m_{j}}$