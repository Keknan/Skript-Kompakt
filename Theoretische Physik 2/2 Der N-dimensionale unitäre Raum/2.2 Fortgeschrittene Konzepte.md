***

Wir verlasen erstmal die Dirac Notation und machen mit Kommutatoren weiter. Kommutatoren sind Operatoren, wie Matrizen, die durch

$$
[\mathbf{A},\mathbf{B}]:=\mathbf{AB}-\mathbf{BA}
$$

definiert sind. D.h. die Operatoren selbst bilden einen Operator. Dieses Konzept verallgemeinern wir , indem wir eine Funktion $f$ betrachten, die von einer komplexen Variable $z$ abhängt. Um uns auch auf eine allgemeine Funktion zu "beschränken", nutzen wir die Potenzreihendarstellung

$$
f(z)=\sum_{n=0}^{\infty}a_{n}z^{n}
$$

Konvergenzradien und unstetige Funktionen, die nicht entwickelbar sind ignorieren wir. Ähnlich wie beim Matrixexponential kann auch diese Funktion einen Operator $\mathbf{A}$ des $N$-dimensionalen unitären Raum als Variable annehmen, die Funktion $f(\mathbf{A})$ sieht dann so aus:

$$
f(\mathbf{A}):=\sum_{n=0}^{N}a_{n}\mathbf{A}^{n}
$$

Es gilt $\mathbf{A}^{0}=\mathbf{1}$ und $\mathbf{A}^{n}$ die $n$-fache Matrixmultiplikation mit dem Operator selbst. Ohne Beschränkung der Allgemeinheit gelten die bisher hergeleiteten Potenzreihen bekannter Funktionen bereits. Das Operatorexponential ist also wie gewohnt

$$
f(\mathbf{A})=\sum_{n=0}^{\infty} \frac{\mathbf{A}^{n}}{n!}=\exp(\mathbf{A})=e^{\mathbf{A}}
$$

Hier ist natürlich zu beachten, dass im skalaren Fall die $z$ kommutieren, Operatoren kommutieren allgemein aber nicht. Aufpassen. Diese Funktion lässt sich auch ableiten, jedoch nicht nach $\mathbf{A}$ selbst, sondern nur im Fall, dass $\mathbf{A}$ von einem Parameter $p$ abhängt. Die Ableitung sieht dann aus wie unser gewohnte Differenzenquotient

$$
\frac{\text{d}\mathbf{A}(p)}{\text{d}p}:=\lim_{ \Delta p \to 0 } \frac{\mathbf{A}(p+\Delta p)-\mathbf{A}(p)}{\Delta p} 
$$

Warum habe ich erwähnt, dass wir berücksichtigen müssen, dass Operatoren allgemein nicht Kommutieren? Nutzen wir ein Beispiel, ein sehr wichtiges sogar, nämlich $\mathbf{A}(p):=e^{p\mathbf{B}}$. Über die Kettenregel (bzw. der Potenzreihendarstellung) können wir einen wichtigen Zusammenhang finden:

$$
\frac{\text{d}\mathbf{A}(p)}{\text{d}p}=\mathbf{B}e^{p\mathbf{B}}=e^{p\mathbf{B}}\mathbf{B=\mathbf{B}}\mathbf{A}(p)=\mathbf{A}(p)\mathbf{B}
$$

Wir *dürfen* hier kommutieren, da $\exp(p\mathbf{B})$ nur Potenzen von $\mathbf{B}$ besitzt, und $\mathbf{B}$ offensichtlich mit sich selbst kommutiert. 

Als nächstes wollen wir herausfinden, ob wenn es zwei Operatoren $\mathbf{A},\mathbf{B}$ gibt, und diese selbstadjungiert, als auch kommutierend sind (also $[\mathbf{A},\mathbf{B}]=0$), diese auch gemeinsame Eigenvektoren besitzen. Dabei betrachten wir den Fall, wo mindestens eine Matrix nicht entartet ist, wir wählen dafür einfach $\mathbf{A}$. Diese Matrix soll also zu jedem Eigenwert $a_{j}$ einen eindeutigen Eigenvektor $\ket{a_{j}}$ besitzen. Wir nutzen

$$
[\mathbf{A},\mathbf{B}]=0 \implies \mathbf{AB}=\mathbf{BA}
$$

und multiplizieren beide Seiten mit $\ket{a_{j}}$:

$$
\mathbf{AB}\ket{a_{j}}=\mathbf{BA}\ket{a_{j}}=a_{j}\mathbf{B}\ket{a_{j}}  
$$

Interessanterweise können wir nun $\mathbf{B}\ket{a_{j}}$ als einen weiteren Vektor auffassen, der mit $\mathbf{A}$ multipliziert wird, und wir erkennen, dass dieser den Eigenwert, mal jenen Vektor entspricht! D.h. $\mathbf{B}\ket{a_{j}}$ ist ein Eigenvektor zu $\mathbf{A}$ zum Eigenwert $a_{j}$. Des Weiteren: Der Eigenvektor ist aber eindeutig festgelegt, dass bedeutet, dass nach der Multiplikation mit $\mathbf{B}$ und Multiplikation mit $a_{j}$ nur ein neuer Vorfaktor entsteht, es gilt also:

$$
\mathbf{B}\ket{a_{j}}=b_{j}\ket{a_{j}}  
$$

Aber das heißt doch, dass $\ket{a_{j}}$ auch ein Eigenvektor zu $\mathbf{B}$ ist! Darum haben wir auch geschickt $b_{j}$ als Vorfaktor gewählt, weil dieser Eigenwert zu $\mathbf{B}$ ist. Und im nicht entarteten Fall bilden die $\ket{a_{j}}$ einen Eigenraum, der die Matrizen in dieser Basis eindeutig festlegt:

$$
\begin{align}
\mathbf{A}&=\sum_{j=1}^{N}a_{j} \ket{a_{j}} \bra{a_{j}} \\
\mathbf{B} & =\sum_{j=1}^{N}b_j \ket{a_{j}} \bra{a_{j}}  \\
\mathbf{1}&=\sum_{j=1}^{N} \ket{a_{j}} \bra{a_{j}} 
\end{align}
$$

Nun müssen wir aber auch den anderen Fall betrachten. Sagen wir beide Operatoren besitzen entartete Eigenwerte, um auch zu wissen, wie "stark" ein Eigenwert $a_{j}$ (Wir betrachten erstmal nur $\mathbf{A}$) entartet, geben wir ihm einen **Entartungsgrad** $M_{j}\in \mathbb{N}$, es existieren also $n<N$ verschiedene Eigenwerte $a_{j}$ und jeder besitzt einen Entartungsgrad $M_{j}$ mit $j=1,2,\dots,n$. Ähnlich wie wir es in HMII mit Hauptvektoren zu einem Eigenwert $\lambda$ gemacht haben, in dem die geometrische Vielfachheit ungleich der algebraischen Vielfachheit ist, kann dieser Eigenwert einen Unterraum $\mathcal{H}_{j}$ zum Eigenwert $a_{j}$ aufspannen. Dieser besitzt dann $M_{j}$ Dimensionen (z.B. Entartungsgrad $M_{1}=3$, dann kann $a_{1}$ drei unterschiedliche Eigenvektoren erzeugen). Das bedeutet wiederum, dass wir auf eine Art Jordan-Block stoßen werden, wenn wir uns die Spektraldarstellung [[2.1 Grundlagen und die Dirac-Notation#^a789c9]] anschauen. Wir müssen also bei unserem Eigenvektor $\ket{a_{j}}$ erkennbar machen, der *wie vielte Eigenvektor er ist*. Und da ist der Physiker faul und schreibt einfach $\ket{a_{j},m_{j}}$. WICHTIG! $m_{j}$ ist ein eigener Parameter und läuft von $m_{j}=1,2,\dots,M_{j}$! 

Die Spektraldarstellung im entarteten Fall sieht also so aus:

$$
\mathbf{A}=\sum_{j=1}^{n}\sum_{m_{j}=1}^{M_{j}} a_{j} \ket{a_{j},m_{j}} \bra{a_{j},m_{j}} 
$$

Dies ist natürlich auch eine Diagonalmatrix, diese sieht aber so aus:

$$
\mathbf{A}\dot{=}
\begin{pmatrix}
a_{1} & 0 & 0 & \dots & \dots & \dots & \dots & \dots & 0 \\
0 & a_{1} & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
\dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & a_{1} & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & a_{2} & \dots & \dots & \dots & \dots \\
. & . & . & . & . & . & . & . & . \\
. & . & . & . & . & . & . & . & . \\
. & . & . & . & . & . & . & . & . \\
0 & 0 & 0 & \dots & \dots & \dots & \dots & \dots & a_{n}
\end{pmatrix}
$$

Nun gehen wir analog vor, um herauszufinden, wie $\ket{a_{j},m_{j}}$ auf $\mathbf{B}$ wirkt:

$$
\mathbf{AB}\ket{a_{j},m_{j}} =\mathbf{BA}\ket{a_{j},m_{j}}=a_{j}\mathbf{B}\ket{a_{j},m_{j}} 
$$

$\mathbf{B}\ket{a_{j},m_{j}}$ ist also auch Element von $\mathcal{H}_{j}$ zu $a_{j}$. Für unterschiedliche Eigenwerte gilt wiederum, dass z.B. $\ket{a_{k},m_{k}}$ orthogonal auf $\mathbf{B}\ket{a_{j},m_{j}}$  steht, also

$$
\bra{a_{k},m_{k}}\mathbf{B}\ket{a_{j},m_{j}}=0  
$$

Diese Unterräume $\mathcal{H}_{j}$ mit Entartungsgrad $M_{j}$ (die Dimension) lassen sich durch $M_{j}\times M_{j}$ Matrizen darstellen. In der Basis $\{ \ket{a_{j},m_{j}} \}$ existiert also in jedem Unterraum ein geeignetes $B_{j}$, welches kets aus diesem Unterraum geeignet "arrangiert", sodass sie, angewandt auf $\mathbf{A}$ ein Eigenvektor sind. Wir erhalten also eine Blockstruktur

$$
\mathbf{B} \dot{=}
\begin{pmatrix}
B_{1} & 0 & \dots & 0 \\
0 & B_{2} & \dots & 0 \\
. & . & . & . \\
0 & 0 & \dots & B_{n}
\end{pmatrix}
$$

Dieser Operator ist selbstadjungiert, würde man diese Struktur ausschreiben, erkennt man sofort, dass dadurch auch $B_{j}$ selbstadjungiert sein muss. Uns ist es nun möglich für jeden Unterraum $\mathcal{H}_{j}$ - und damit auch $B_{j}$ - in der Basis $\{ \ket{a_{j},1},\ket{a_{j},2},\dots,\ket{a_{j},M} \}$ eine neue Basis aus Eigenvektoren zu finden, wodurch die Matrix $\mathbf{B}$ diagonalisierbar ist. An der Struktur von $\mathbf{A}$ ändert sich natürlich nichts, da $\mathbf{A}$ ein linearer Operator auf keinem der Unterräume $\mathcal{H}_{j}$ ist. Natürlich gilt auch, dass wenn der Entartungsgrad $M_{j}=1$ ist, so ist $B_{j}$ eindeutig festgelegt - es existiert kein Basiswechsel in $\mathcal{H}_{j}$. Um nun jedes $B_{j}$ zu diagonaliseren, also in eine EV Basis zu bringen, müssen wir auch seine EW $b_{k}$ bestimmen. Wir bezeichnen die Vektoren dieser neuen Basis $\{ \ket{a_{j},b_{k}} \}$. $\mathbf{A},\mathbf{B}$ lassen sich dann in der Spektraldarstellung

$$
\begin{align}
\mathbf{A}&=\sum_{j,k}a_{j}\ket{a_{j},b_{k}} \bra{a_{j},b_{k}}  \\
\mathbf{B}&= \sum_{j,k} b_{k} \ket{a_{j},b_{k}} \bra{a_{j},b_{k}} 
\end{align}
$$

darstellen. Laut Vollständigkeitsrelation ist das ket-bra Produkt die Identität $\mathbf{1}$. 

Oben beschrieben ist der Fall, dass *$B_{j}$ im Unterraum $\mathcal{H}_{j}$ keine entarteten EW hat*. Sollte dies doch der Fall sein, so existieren Vektoren im Unterraum $\mathcal{H}_{jk}$, die EV von $\mathbf{B}$ mit EW $b_{k}$ sind, aber auch gleichzeitig EV von $\mathbf{A}$ zum EW $a_{j}$ (ja es ist verwirrend). Wir gehen dann so ähnlich vor wie oben, indem wir uns den Operator $\mathbf{C}$ zuziehen, welcher mit $\mathbf{A},\mathbf{B}$ vertauschen soll. Dann errichten wir eine neue Basis in $\mathcal{H}_{jk}$ aus EV von $\mathbf{C}$ die die EW $c_{l}$ besitzen. Sind nun diese EW nicht entartet, so besitzen wir die eindeutige Basis $\{ \ket{a_{j},b_{k},c_{k}} \}$ und die Darstellung

$$
\begin{align}
\mathbf{A}&=\sum_{j,k,l}a_{j}\ket{a_{j},b_{k},c_{l}} \bra{a_{j},b_{k}c_{l}}  \\
\mathbf{B}&= \sum_{j,k,l} b_{k} \ket{a_{j},b_{k},c_{l}} \bra{a_{j},b_{k},c_{l}}  \\
\mathbf{C}&=\sum_{j,k,l} c_{k} \ket{a_{j},b_{k},c_{l}} \bra{a_{j},b_{k},c_{l}} 
\end{align}
$$

Und wie man sich denken kann, sind manche $c_{l}$ entartet, dann müssen wir einen neuen Operator, der paarweise vertauscht hinzufügen. Wir errichten also in **jedem Fall** ein Satz komutierender Operatoren $\{ \mathbf{A},\mathbf{B},\mathbf{C},\dots \}$, die die gemeinsame Basis aus EV $\{ \ket{a_{j},b_{k},c_{l},\dots} \}$ besitzen sollen. Dann nennen wir die Menge jener Operatoren **einen vollständigen Satz vertauschbarer kommutierender Eigenvektoren**.

Auch wenn dieses Kapitel etwas gewaltig ist, werden wir diese Konzepte auskosten, besonders im nächsten Kapitel...