***

Wir werden nun diese neue Sprache an einem uns bekannten Beispiel festmachen: den Polarisatoren. Die ebene Welle lässt sich Teilchenbild als einen Strahl von Photonen darstellen, die jeweils den Polarisationszustand $\ket{\psi}$ besitzen. Da die Polarisation eine Linearkombination aus Ausschlägen in $x$- oder $z$-Richtung ist, und sowohl komplexe Zahlen besitzen kann (z.B. lässt sich die Welle als $\exp(ix)$ darstellen), liegen wir im $\mathbb{C}^{2}$, welcher ein 2D unitärer Vektorraum ist. Wir stellen also $\ket{\psi}=\psi_{1}\ket{1}+\psi_{3}\ket{3}$ ($\psi_{1},\psi_{2}\in \mathbb{C}$) dar. Simpel, sieht fast so aus wie im Wellenbild, und lässt sich mit Dirac

$$
\begin{align}
\braket{ 1 | 1 } &=\braket{ 3 | 3 } =1 \\
\braket{ 1 | 3 } &=0
\end{align}
$$

vereinbaren. Wir werden in der Regel normierte kets benutzen. Die Physiker beschreiben nun den gesamten Zustand $\ket{\psi}$ als auf $1$ genormt (kennen wir auch analog zu Polarisation), allerdings berechnen sie es etwas anders, da wir uns auf der komplexen Zahlenebene befinden. Statt einfach den Betrag $|\braket{ \psi | \psi} |$ zu bilden, müssen wir das noch **quadrieren**, da dieser Ausdruck sehr wohl negativ sein kann. Eine Negative "Länge" würde keinen Sinn ergeben. Eine kräftigere Begründung, warum das so sein muss folgt.

Wir setzen also $|\braket{ \psi | \psi }|^{2}=|\psi_{1}|^{2}+|\psi_{3}|^{2}=1$ fest. Im Vergleich zur Welle $E_{1}\vec{e}_{1}+E_{3}\vec{e}_{3}$ übernehmen die Kets $\ket{1},\ket{3}$ die Richtungsvektoren, unsere $\psi_{i}$ müssen also in einer Relation $\psi_{i}=cE_{i}$ mit $i=1,3$ und $c\in \mathbb{C}$ stehen. Beide sollen mit der gleichen Konstante $c$ gestreckt werden, um die Normiertheit zu bewahren. Wir erhalten also $|c|=(|E_{1}|^{2}+|E_{3}|^{2})^{-1/2}$.

Wo ist die Frage der ursprünglichen Welle? Im Wellenbild, lässt sich die Welle komplex darstellen, mit dem Phasenterm $e^{i\phi}$, bildet man nun aber den Betrag von ihr - so wie oben - verschwindet sie einfach. Tada. Wir können also ohne Probleme sagen, $\phi=0$, wodurch auch $c$ reell wird. Wir erhalten also

$$
\ket{\psi}=\psi_{1}+\ket{1} +\psi_{3}\ket{3}=\frac{E_{1}}{\sqrt{ |E_{1}|^{2}+|E_{3}|^{2} }}  \ket{1}+ \frac{E_{3}}{\sqrt{ |E_{1}|^{2}+|E_{3}|^{2} }} \ket{3}  
$$

Die einzelnen $E_{i}$ sind die Amplituden der Felder in ihrer jeweiligen Richtung, die damit auch gleichzeitig eine Energiedichte darstellen. Die einzelnen gebastelten Koeffizienten geben also an, **wie viel der Gesamtenergie durchtritt** nachdem man einen Polarisator durchquert. Und da wir auf $1$ normiert sind, gibt es eine **Prozentzahl** an. Trifft unser $\ket{\psi}$ auf einen $x$-Polarisator, so überlebt der erste Koeffizient, und gibt den Anteil der ursprünglichen Gesamtenergie an. 

Was machen dann unsere Teilchen? Unsere Photonen stehen in Superposition der Zustände $\ket{1}$ und $\ket{3}$. Trifft nun ein Teilchen auf einen $x$- oder $z$-Polarisator, so "entscheidet es sich" durchzutreten oder nicht. Wir haben es also mit **Wahrscheinlichkeiten** $w_{x/z}(\ket{\psi})$ zutun, und diese sind genau ihre Koeffizienten.

WICHTIG!!! Jedes Photon ist im **gleichen Zustand $\ket{\psi}$ präperiert!** Wir würden in unserem Gedankenexperiment also das gleiche Photon mehrmals auf den Polarisator schicken und merken, dass es manchmal passiert und manchmal nicht, und das tun wir unendlich mal!

Um also die WK herauszuprojizieren, so brauchen wir nur das Skalarprodukt mit dem jeweiligen Zustand bilden. Aber auch hier Achtung: die Koeffizienten können sehr wohl negativ sein, und die Summe ihrer Betragsquadrate $1$, also bilden wir dafür

$$
w_{x}(\ket{\psi} )=|\braket{ 1 | \psi } |^{2}\quad,\quad w_{z}(\ket{\psi} )=|\braket{ 3 | \psi } |^{2}
$$

Das Skalarprodukt an sich nennen wir **Wahrscheinlichkeitsamplitude**. 

Natürlich passiert das Photon nicht nur $x$- oder $z$- polarisierte Filter - sie können ja irgendwie rotiert sein. Drehen wir den Polarisator um einen Winkel $\varphi$

![[Pasted image 20250506215834.png]]
Abb.: links die Zustände des Photons als ONS, rechts, analogon mit Polarisatoren

Wie oben gezeigt, lässt sich dieser rotierte Zustand $\ket{1'}$ (Man kann genauso $\ket{3'}$ bilden, es spielt keine Rolle) durch die Grundzustände $\{ \ket{1},\ket{3} \}$ darstellen, wir haben also quasi unseren ersten Basiswechsel betrieben. Formal lautet $\ket{1'}$

$$
\ket{\psi}=\ket{1'}=\cos \varphi \ket{1}+\sin \varphi \ket{3}    
$$

^d3384e

Die WK des Photons im Zustand $\ket{1'}$ durch den $x$-Polarisator zu treffen lautet dann

$$
w_{x}(\ket{1'} )=|\braket{ 1 |1'  } |^{2}=\cos ^{2}\varphi
$$

Was wir oben gemacht haben ist tatsächlich nicht ganz korrekt, doch hilft uns das Problem erstmal zu verstehen. Das Photon besteht immer noch nur aus **Zuständen** die als kets $\ket{\psi}$ dargestellt werden, und befindet sich dann im Polarisationszustand $\ket{1'}$. Träfe dieses Photon nun auf einen Polarisator im exakt gleichen Zustand, wir nennen diesen $\ket{P}$, so besitzt es Durchgangswahrscheinlichkeit $1$. Allgemein berechnen wir also die WK durchzugehen mit $|\braket{ P | \psi }|^{2}$. Das Drehen des Polarisators gibt also nicht den Zustand von $\ket{\psi}$ an, doch dieser wird irgendwie durch [[#^d3384e]] dargestellt.

Nach dem Durchgang ist das Photon definitiv im Zustand $\ket{P}$, dies können wir bestätigen indem wir einen zweiten Polarisator im Zustand $\ket{P}$ dahinter stellen würden, 100% der Photonen träfen durch.

Das Betragsquadrat $|\braket{ \dots | \dots }|^{2}$ als **Übergangswahrscheinlichkeit** ist im Übrigen eins der zentralen Postulate der QM. Diese gibt an, wo das Photon sein *könnte*, ähnlich wie das Interferenzmuster beim Doppelspalt eine WK Verteilung angibt. Eine etwas unzufriedenstellende Erkenntnis, dass obwohl wir $\ket{\psi}$ "kennen" nicht wissen können, wo es am Ende sein wird. Wir können uns bisher nur Gedanken machen, welche Eigenschaften zu diesem Verhalten führen (z.B. in diesem Fall die Polarisation). Wir stellen jedes Teilchen (Photon) nämlich als eine Linearkombination dar, d.h. sie ist i.a. weder $x$- noch $z$- polarisiert, was dem klassischen Analogon vollkommen widerspricht. Daher gibt es hier auch so wenig Bilder, weil es beinahe unmöglich ist diesen QZ darzustellen.

##### Stern-Gerlach: Revisited
***

Wir machen weiter mit dem SG. Betrachten wir das Experiment, so sind wir in der Lage die Atome wieder durch zwei Zustände zu beschreiben, die eine Linearkombination ist: Die Ablenkung nach oben $\ket{\uparrow}$ und nach unten $\ket{\downarrow}$. Mit dieser Schreibweise verlieren wir aber Informationen über die Richtung des SG, daher schreiben wir lieber $\ket{\uparrow}\to \ket{\vec{e}_{3},+}$ für den oberen Strahl und $\ket{\downarrow}\to \ket{\vec{e}_{3},-}$ für den unteren, wenn der SG in $\vec{e}_{3}$ Richtung ausgerichtet ist. Zeigt er in die beliebige Richtung $\vec{n}$ dann erden die Vektoren zu $\ket{\vec{n},\pm}$. 

Wir schreiben also analog $\ket{\psi}=\psi_{+}\ket{\vec{n},+}+\psi_{-}\ket{\vec{n},-},\quad \psi_{\pm}\in \mathbb{C}$ und $|\psi_{+}|^{2}+|\psi_{-}|^{2}=1$ auf, wieder ein zweidimensionaler Vektor im unitären Raum $\mathbb{C}^{2}$. Diese Zustände treten ja nicht einfach so auf, sondern entstehen aus dem diskreten Energiespektrum gemäß $-\vec{\mu}\cdot \vec{B}$. Da dieses Skalarprodukt im Experiment nur zwei Werte erzeugt, schreiben wir einfach $\mp \mu_{0}B$. 

Mathematisch können wir die Wechselwirkung im $B$-Feld durch einen linearen Operator darstellen, dabei wirkt das $B$-Feld immer noch klassisch, die diskreten Werte sind ja oben vom magnetischen Moment $\vec{\mu}$ bestimmt. D.h. nicht das Magnetfeld, sondern $\vec{\mu}$ wird zu einem Operator!

Bisher stellen wir $\vec{\mu}$ aber als einen (kartesischen) Vektor dar, welcher an sich kein Operator sein kann, genauer müssen die Komponenten $\mu_{i}$ zu Operatoren werden. Falls es dich verwirrt: Unser Ziel wird es sein für jede Komponente $\mu_{i}$ wieder "eine $2\times 2$ Matrix" (Physikalisch richtiger wäre ein linearer Operator mit Dimension 2) zu konstruieren, die dann in der Lage ist mit dem zweidimensionalen Vektor $\ket{\psi}$ operieren kann. Das Skalar $\mu_{0}$ können wir nun noch aus dem Vektor hinausziehen: $\vec{\mu}=\mu_{0}\vec{\sigma}$, das gleiche machen wir mit $\vec{B}=B\vec{n}$. Bilden wir nun das Skalarprodukt, so erhalten wir

$$
-\mu_{0}B\underbrace{ (\sigma_{1}n_{1}+\sigma_{2}n_{2}+\sigma_{3}n_{3}) }_{ =:\sigma_{\vec{n}} }
$$

Dieser Ausdruck beschreibt also eine Art Gesamtenergie des Teilchens (die kinetische Energie wird ignoriert), daher können wir eine Art **Hamiltonfunktion** $H$ bauen, die wir hier aber **Hamiltonoperator** nennen werden. Die $\sigma_{i}$ sind natürlich nun die zu bestimmenden Operatoren, konstruieren sie zu unserem Gefallen. Wir fordern z.B., dass ein $\ket{\vec{n},\pm}$ (Wir nennen diese Vektoren **Eigenzustände**) angewandt auf das $\sigma_{\vec{n}}$ wieder den gleichen Zustand $\ket{\vec{n},\pm}$ ergeben (Ein Silberatom in diesem Zustand wird nach dem Passieren des selben SG definitiv in diesem Zustand bleiben). Dies hat die Struktur der bekannten Eigenwerte, die hier $\pm 1$ sind (Es geht ja nur um die Richtung, $+$ wenn's nach oben geht $-$ nach unten). Das bedeutet also

$$
\sigma_{\vec{n}}\ket{\vec{n},\pm}=\pm \ket{\vec{n},\pm} 
$$

^5281bc

Multiplizieren wir diesen Ausdruck mit $-\mu_{0} B$ können wir bestätigen, dass $H$ auch ein Operator ist:

$$
H \ket{\vec{n},\pm}=\mp \mu_{0}B\ket{\vec{n},\pm} 
$$

Aus diesen paar Schritten erkennen wir eine Menge:

1. Die Energie ist quantisiert mit Werten $\pm \mu_{0}B$
2. Die gemessenen Energien sind EW von $H$
3. Messen wir nach Passieren des Atoms bspw. die Energie $-\mu_{0}B$ so ist $\ket{\psi}=\ket{\vec{n},+}$ 

Nun haben wir auch alle nötigen Zutaten, um $\sigma_{\vec{n}}$ in Spektraldarstellung zu bringen:

$$
\sigma_{\vec{n}}=\sum_{\nu=+,-} \nu \ket{\vec{n},\nu} \bra{\vec{n},\nu}= \ket{\vec{n},+} \bra{\vec{n},+} - \ket{\vec{n},-} \bra{\vec{n},-} 
$$

^8ffc99

Das Ding können wir auch mit $-\mu_{0}B$ multiplizieren um $H$ darzustellen.

$$
H=-\mu_{0}B(\ket{\vec{n},+} \bra{\vec{n},+} - \ket{\vec{n},-} \bra{\vec{n},-} )
$$

Dabei ist das Produkt $\ket{\vec{n},\pm} \bra{\vec{n},\pm}$ unser neuer Projektor, und weil wir bereits gezeigt haben, dass diese selbstadjungiert sind, ist auch $\sigma_{\vec{n}}^{\dagger}=\sigma_{\vec{n}}$. Das gilt natürlich auch für $H$.

Hieraus gewinnen wir auch gleichzeitig eine Strategie zur Gewinnung der Energiemesswerte. Diese sind nämlich die EW des Hamilton Operators, wie wir diesen aus der Hamilton Funktion gewinnen bleibt uns aber noch offen. Messen wir einen Energiewert $E_{n}$ eines Teilchens, so können wir darauf schließen, dass das Teilchen im Zustand $\ket{E_{n}}$ ist und $E_{n}$ sein Eigenwert ist.

![[Pasted image 20250507161544.png]]

Wir nennen $H$ aber nicht ohne Grund den Hamilton Operator. Wir wissen bisher, dass er über seine EW die Energiezustände festlegt, kann aber auch wie bei der Hamiltonfunktion$\mathcal{H}$ die **Dynamik** des Systems beschreiben. Dafür brauchten wir die Freiheitsgrade des Anfangszustands eines Massenpunktes, doch die Freiheitsgrade sind uns für den Hamiltonoperator unbekannt. Außerdem ignorierten wir bisher die zeitliche Entwicklung des Systems, da uns nur die Eigenschaften "nach oben/unten abgelenkt" interessiert haben. 

Die Dynamik beschreiben wir nicht mit den kanonischen Gleichungen, sondern mit der **Schrödingergleichung** die ziemlich schwierig "herzuleiten" ist. Daher nutzen wir lieber ein analogon: Die gekoppelte Hookesche Feder. Die Bewegungsgleichungen lassen sich schnell mit Lagrange/Hamilton herleiten und lauten:

$$
\begin{align}
\frac{\text{d}^{2}}{\text{d}t^{2}}x_{1}(t)&= -\frac{2k}{m}x_{1}(t)+\frac{k}{m}x_{2}(t) \\
\frac{\text{d}^{2}}{\text{d}t^{2}} x_{2}(t)&= \frac{k}{m}x_{1}(t)-\frac{2k}{m}x_{2}(t)  
\end{align}
$$

Das Analogon zum linearen Operator findet dann mit der Matrix statt, und wir schreiben:

$$
\frac{\text{d}^{2}}{\text{d}t^{2}}
\begin{pmatrix}
x_{1}(t)  \\
x_{2}(t)
\end{pmatrix}
=
\begin{pmatrix}
-\frac{2k}{m} & \frac{k}{m} \\
\frac{k}{m} & -\frac{2k}{m}
\end{pmatrix}
\begin{pmatrix}
x_{1}(t) \\
x_{2}(t)
\end{pmatrix}
$$

Die rechte Matrix ist hermitesch und damit selbstadjungiert, und unsere Vektoren $\vec{x}$ sind 2D Vektoren in einem eigenen Raum der generalisierten Koordinaten. Die Lösung $\vec{x}_{0}$ ist natürlich Teil dieses Raums. Um dieses Problem zu lösen würden wir die EW der Matrix bestimmen, und dann die EV bestimmen, die die Moden der Schwingung beschreibt. Jede Schwingung ist dann eine Linearkombination der beiden, unter gegebenen Anfangsbedingungen. Das klingt so ähnlich wie bei uns, Unser ket $\ket{\psi}$ wird durch eine Linearkombination von Eigenzuständen beschrieben, die sich aus den EW eines linearen Operators gewinnen lassen. Nur berücksichtigen wir in dem dynamischen System jetzt noch die Zeit, so lässt sich die Schrödingergleichung

$$
i\hbar \frac{\text{d}}{\text{d}t} \ket{\psi(t)}=H\ket{\psi(t)}  
$$

^bb1186

lesen. Ich war genauso enttäuscht von diesem stumpfen Aufstellen dieser Gleichung, die aus heiterem Himmel kommt, tatsächlich ist die Motivation dahin eine sehr komplizierte, und einen eleganteren Weg gibt es - laut unserem Prof - nicht wirklich.

Wir suchen also nach der Lösung $\ket{\psi(t)}$, wobei wir eine partikuläre Lösung nur für die Anfangswerte $\ket{\psi(0)}$ erhalten, diese sind natürlich in $\psi_{\pm}(0)$ vorgegeben, aber auch hier spielt die Basis eine Rolle! Legen wir uns in $\{ \ket{\vec{e}_{3},+},\ket{\vec{e}_{3},-} \}$ fest, so sieht die Lösung so

$$
\ket{\psi(t)}=\psi_{+}(t)\ket{\vec{e}_{3},+} +\psi_{-}\ket{\vec{e}_{3},-}  
$$

aus. In diesem Fall ist $t$ nicht als ein Parameter der variiert wird, interessanterweise gilt in der QM im allgemeinen für zwei Zeiten $t_{1}\neq t_{2}$ der Fall $\ket{\psi(t_{1})}\neq \ket{\psi_{2}(t)}$. Außerdem brauchen wir nur eine Anfangsbedingung, das ist meistens der "Ort" im klassischen Analogon, in der QM nennen wir das Stumpf $\psi_{\pm}(0)$. Beim gekoppelten Pendel brauchen wir noch die Anfangsgeschwindigkeiten. Hier nicht. Die Lösung der DGL findet aber klassisch über die Entdeckung der EW und zugehörigen EV/EZ statt. 

Diese Gleichung wird unser go to sein um dynamische Systeme quantenmechanisch zu lösen. Sie wird nicht nur für den SG benutzt, sondern i.a. sucht man nach der Hamiltonfunktion aus der man den Hamiltonoperator bilden kann, welcher das System mit [[#^bb1186]] löst.

Wir verlassen diesen Ausflug und beschäftigen uns mit mehr MATHE. Wir erinnern uns daran, dass wir das magnetische Moment in $\mu_{0}\vec{\sigma}$ aufgespalten haben, wobei $\vec{\sigma}$ auf $1$ normiert ist. Um $\sigma_{\vec{n}}$ zu bilden skalarmultiplizieren wir $\vec{n}$, der auch auf $1$ normiert ist. $\sigma_{\vec{n}}$ ist also auch auf $1$ normiert, und dies können wir erkennen, indem wir diesen Ausdruck quadrieren (Wir erhalten in diesem Fall nicht die skalare $1$ sondern die Identität $\mathbf{1}$, da jeder Eintrag ein Operator ist). Wir nutzen [[#^8ffc99]] 

$$
\begin{align}
\sigma_{\vec{n}}^{2}&=(\ket{\vec{n},+} \bra{\vec{n},+}-\ket{\vec{n},-} \bra{\vec{n},-}  )(\ket{\vec{n},+} \bra{\vec{n},+}-\ket{\vec{n},-} \bra{\vec{n},-}  ) \\
&=\ket{\vec{n},+} \bra{\vec{n},+} +\ket{\vec{n},-} \bra{\vec{n},-}  \\
&=\mathbf{1}
\end{align}
$$

Da wir die Eigenschaft der Projektoren ausgenutzt haben. Das gleiche können wir auf dem oben motivierten Weg machen, indem wir stumpf das Skalarprodukt $\vec{\sigma}\cdot \vec{n}$ berechnen und quadrieren

$$
\begin{align}
\sigma_{\vec{n}}^{2}=&\sigma_{1}^{2}n_{1}^{2}+\sigma_{2}^{2}n_{2}^{2}+\sigma_{3}^{2}n_{3}^{2} \\
&+[(\sigma_{1}\sigma_{2}+\sigma_{2}\sigma_{1})n_{1}n_{2}+(\sigma_{1}\sigma_{3}+\sigma_{3}\sigma_{1})n_{1}n_{3}+(\sigma_{2}\sigma_{3}+\sigma_{3}\sigma_{2})n_{2}n_{3}]
\end{align}
$$

Dabei haben wir bereits festgestellt, dass das Skalarprodukt an sich $\mathbf{1}$ ergeben muss, die unteren Terme sollten also festfallen. Dies können wir schreiben als

$$
\sigma_{i}\sigma_{j}+\sigma_{j}\sigma_{i}=2\delta_{ij}\mathbf{1}
$$

^96d5f5

Dabei ist die Reihenfolge wichtig, es handelt sich hier um einen Antikommutator von Operatoren! Wir ziehen uns eine weitere Hilfe zur Seite, und zwar die der Spur der Matrix. Ohne Beweis nehmen wir an, dass die Spur eines Operators gleich der Summer seiner EW ist. Das bedeutet für jedes $\sigma_{i}$, $i=1,2,3$ mit EW $\pm 1$, dass

$$
\text{Sp }\sigma_{i}=0
$$

^92d4ab

Unser letzter Schritt, ist irgendeinen Halt zu finden, um mit [[#^96d5f5]] und [[#^92d4ab]] unsere $\sigma_{i}$ zu bestimmen. Dafür schauen wir uns [[#^5281bc]] an, denn für diesen Fall gilt $\sigma_{i}\ket{\vec{e}_{i},\pm}=\pm \ket{\vec{e}_{i},\pm}$. Für unseren $\vec{e}_{3}$ SG wählen wir also diese Richtung, daher können wir unseren ersten Operator

$$
\sigma_{3} \dot{=} 
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
$$

bilden. Jetzt benutzen wir *noch* eine Eigenschaft der Matrizen, nämlich, dass $\sigma_{j}^{\dagger}=\sigma_{j}$ ist, dessen Spur $0$ ist. D.h. wir können uns die Schablone der restlichen $\sigma_{j}, j=1,2$ bilden via

$$
\sigma_{j} \dot{=}
\begin{pmatrix}
a_{j} & b_{j} \\
b_{j}^{*} & -a_{j}
\end{pmatrix}
$$

Offensichtlich ist $a_{j},b_{j}\in \mathbb{C}$. Nun nutzen wir [[#^96d5f5]], da wir $\sigma_{3}$ kennen, das Kronecker delta ist dann automatisch $\delta_{3j}=0$. Wir berechnen:

$$
\begin{align}
\begin{pmatrix}
a_{j} & b_{j} \\
b_{j}^{*} & -a_{j}
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}+
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
\begin{pmatrix}
a_{j} & b_{j} \\
b_{j}^{*} & -a_{j}
\end{pmatrix}&= \begin{pmatrix}
2a_{j} & 0 \\
0 & 2a_{j}
\end{pmatrix}\\
&=\begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}
\end{align}
$$

(Der Markdown sieht hier übrigens schrecklich aus). Wir können also schonmal das $a_{j}=0$ setzen. Jetzt nutzen wir aus, dass das Quadrat der $\sigma_{j}^{2}=\mathbf{1}$ ergeben soll:

$$
\begin{pmatrix}
0 & b_{j} \\
b_{j}^{*} & 0
\end{pmatrix}
\begin{pmatrix}
0 & b_{j} \\
b_{j}^{*} & 0
\end{pmatrix}
=
\begin{pmatrix}
|b_{j}|^{2} & 0 \\
0 & |b_{j}|^{2}
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$

$b_{j}$ ist nicht $1$! Wir haben es mit einer komplexen Zahl zutun, d.h. die Phase $e^{i\alpha_{j}}$ geht im Betrag verloren. Die Matrix $\sigma_{j}$ besitzt also Nullen auf der Diagonalen und die Phasen auf den Nebendiagonalen. $\alpha_{j}$ kann ja bisher beliebig sein, wir wollen also nun konkretisieren, welcher Wert hier angenommen wird. Wir nutzen [[#^96d5f5]] mit $i=1,j=2$:

$$
\begin{align}
&\begin{pmatrix}
0 & e^{i\alpha_{1}} \\
e^{-i\alpha_{1}} & 0
\end{pmatrix}
\begin{pmatrix}
0 & e^{i\alpha_{2}} \\
e^{-i\alpha_{2}} & 0
\end{pmatrix}
+
\begin{pmatrix}
0 & e^{i\alpha_{2}} \\
e^{-i\alpha_{2}} & 0
\end{pmatrix}
\begin{pmatrix}
0 & e^{i\alpha_{1}} \\
e^{-i\alpha_{1}} & 0
\end{pmatrix} \\
&=
\begin{pmatrix}
2\cos(\alpha_{1}-\alpha_{2}) & 0 \\
0 & 2\cos(\alpha_{1}-\alpha_{2})
\end{pmatrix}=
\begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}
\end{align}
$$

Die Differenzen der Phasen müssen also $\alpha_{1}-\alpha_{2}=\pi /2+n\pi,n\in \mathbb{Z}$ ergeben. Die Differenz ist also nur entscheidend, aber beliebig. Der Einfachheit halber setzen wir $\alpha_{1}=0$, auf dem Einheitskreis muss dann $\alpha_{2}=\mp i$ sein ($\mp$ statt $\pm$ wegen dem Minus in der Gleichung). Wir nehmen einfach $e^{i\alpha_{2}}=-i$, das ist Konvention. Wir kennen also unsere **Pauli-Spinmatrizen**! Die sind entscheidend, und lauten nun in der Basis $\{ \ket{\vec{e}_{3},+},\ket{\vec{e}_{3},-}\}$ lauten sie:

$$
\sigma_{1} \dot{=}
\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}
\quad
\sigma_{2} \dot{=}
\begin{pmatrix}
0 & -i \\
i & 0
\end{pmatrix}
\quad
\sigma_{3} \dot{=}
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
$$

Für eine Basis in den restlichen kartesischen Richtungen gehen wir analog vor und erkennen, dass die Matrizen zyklisch sind. Das kannst du gerne selber machen, der Lösungsweg steht ja hier. Bleiben wir in dieser Basis, aber drehen den SG in die beliebige Richtung $\vec{n}=\sin \theta \cos \varphi \vec{e}_{1}+\sin \theta \sin \varphi \vec{e}_{2}+\cos \theta \vec{e}_{3}$, und wollen $\sigma_{\vec{n}}$ bestimmen, dann bilden wir einfach das Produkt

$$
\sigma_{\vec{n}}=\vec{\sigma}\cdot \vec{n}=
\begin{pmatrix}
\cos \theta & e^{-i\varphi}\sin \theta \\
e^{i\varphi} & -\cos \theta
\end{pmatrix}
$$

Wir erkennen hier sofort, dass unsere Ursprünglichen annahmen gelten: In $\vec{e}_{3}$ gedreht erhalten wir nur $\sigma_{3}$, etc. Wir können zeigen, dass die Spalten